{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":39251.489594,"end_time":"2024-10-22T03:37:55.329250","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-21T16:43:43.839656","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1c5a24d5a2fa492da0189c425c18399e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22e6f70c582e4ed2a495fde62d1e29ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce97a8fda9546c8bdc8e4478dd36403","max":1115567652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aec0175f281f42efbebff60eeec21e63","value":1115567652}},"41b6850ce48b4e1f9c473357048d5a37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d48bea4f9ecc49a293cfa913598a3c4e","placeholder":"​","style":"IPY_MODEL_81102b4e31534a82bd7be3a5393df4f7","value":" 1.12G/1.12G [00:03&lt;00:00, 324MB/s]"}},"41feb7b2b3dd41209bd44d53ba26f818":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdf131e023b54e928895c848376ae25f","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba8568cc38c547b3baed652219195fdc","value":615}},"5c7f118ee6984cd981751d31da8d5817":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdddd0aec2794092a90d22aaa7ebf7a1","IPY_MODEL_22e6f70c582e4ed2a495fde62d1e29ba","IPY_MODEL_41b6850ce48b4e1f9c473357048d5a37"],"layout":"IPY_MODEL_883e134dc03b47648e55db0dc1259551"}},"81102b4e31534a82bd7be3a5393df4f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"883e134dc03b47648e55db0dc1259551":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ab1d35cf5cc4210a03e2a1ed7cde538":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ce97a8fda9546c8bdc8e4478dd36403":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dbcb722a1764a29b31cacd1bfcde359":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aec0175f281f42efbebff60eeec21e63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0a679d839384d8d9454ff918c42957a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b69b7b87adc54962a628e2bdefeeedf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b65c5b787d4236a5dcf0293e7afca6","placeholder":"​","style":"IPY_MODEL_b0a679d839384d8d9454ff918c42957a","value":" 615/615 [00:00&lt;00:00, 48.3kB/s]"}},"ba8568cc38c547b3baed652219195fdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc37579f889b40bea387b256bd281d31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdddd0aec2794092a90d22aaa7ebf7a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c5a24d5a2fa492da0189c425c18399e","placeholder":"​","style":"IPY_MODEL_9dbcb722a1764a29b31cacd1bfcde359","value":"model.safetensors: 100%"}},"be0e339319f2498bb2326b1315dadff4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3b65c5b787d4236a5dcf0293e7afca6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d48bea4f9ecc49a293cfa913598a3c4e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec1981b28e2546b798d34497e242ba9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8805497e0ae4112b26492c0cf2e8a15","IPY_MODEL_41feb7b2b3dd41209bd44d53ba26f818","IPY_MODEL_b69b7b87adc54962a628e2bdefeeedf2"],"layout":"IPY_MODEL_be0e339319f2498bb2326b1315dadff4"}},"f8805497e0ae4112b26492c0cf2e8a15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ab1d35cf5cc4210a03e2a1ed7cde538","placeholder":"​","style":"IPY_MODEL_bc37579f889b40bea387b256bd281d31","value":"config.json: 100%"}},"fdf131e023b54e928895c848376ae25f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-22T07:44:07.630131Z","iopub.execute_input":"2024-10-22T07:44:07.630586Z","iopub.status.idle":"2024-10-22T07:44:12.312405Z","shell.execute_reply.started":"2024-10-22T07:44:07.630545Z","shell.execute_reply":"2024-10-22T07:44:12.311450Z"},"papermill":{"duration":11.309588,"end_time":"2024-10-21T16:43:57.861364","exception":false,"start_time":"2024-10-21T16:43:46.551776","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nhuggingface_token = \"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\"\nlogin(token=huggingface_token)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:44:12.917625Z","iopub.execute_input":"2024-10-22T07:44:12.918002Z","iopub.status.idle":"2024-10-22T07:44:13.005763Z","shell.execute_reply.started":"2024-10-22T07:44:12.917966Z","shell.execute_reply":"2024-10-22T07:44:13.004839Z"},"papermill":{"duration":0.587427,"end_time":"2024-10-21T16:43:58.467021","exception":false,"start_time":"2024-10-21T16:43:57.879594","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:06.257708Z","iopub.execute_input":"2024-10-22T07:45:06.258096Z","iopub.status.idle":"2024-10-22T07:45:06.414658Z","shell.execute_reply.started":"2024-10-22T07:45:06.258060Z","shell.execute_reply":"2024-10-22T07:45:06.413594Z"},"papermill":{"duration":0.171722,"end_time":"2024-10-21T16:43:58.655243","exception":false,"start_time":"2024-10-21T16:43:58.483521","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:06.416214Z","iopub.execute_input":"2024-10-22T07:45:06.416495Z","iopub.status.idle":"2024-10-22T07:45:06.421104Z","shell.execute_reply.started":"2024-10-22T07:45:06.416465Z","shell.execute_reply":"2024-10-22T07:45:06.420160Z"},"papermill":{"duration":0.023555,"end_time":"2024-10-21T16:43:58.695433","exception":false,"start_time":"2024-10-21T16:43:58.671878","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])\ndf['description'] = df['description'] + ' ' + df['retailer'] # retailer is beaing added","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:06.709655Z","iopub.execute_input":"2024-10-22T07:45:06.710039Z","iopub.status.idle":"2024-10-22T07:45:06.752692Z","shell.execute_reply.started":"2024-10-22T07:45:06.710002Z","shell.execute_reply":"2024-10-22T07:45:06.751670Z"},"papermill":{"duration":0.853826,"end_time":"2024-10-21T16:43:59.565431","exception":false,"start_time":"2024-10-21T16:43:58.711605","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, XLMRobertaModel\nfrom torch.distributions import Categorical\nimport numpy as np\nfrom typing import Dict, List, Union, Tuple\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:32.773728Z","iopub.execute_input":"2024-10-22T07:45:32.774731Z","iopub.status.idle":"2024-10-22T07:45:34.314991Z","shell.execute_reply.started":"2024-10-22T07:45:32.774677Z","shell.execute_reply":"2024-10-22T07:45:34.313971Z"},"papermill":{"duration":1.73321,"end_time":"2024-10-21T16:44:01.315254","exception":false,"start_time":"2024-10-21T16:43:59.582044","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For using the new one\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n\n# For loading the saved one\n# model_dir = \"/kaggle/input/new-transformer-experiment-12-emb-xlm-roberta-tmp/New_model\"\n# tokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:34.316631Z","iopub.execute_input":"2024-10-22T07:45:34.317100Z","iopub.status.idle":"2024-10-22T07:45:37.698658Z","shell.execute_reply.started":"2024-10-22T07:45:34.317065Z","shell.execute_reply":"2024-10-22T07:45:37.697556Z"},"papermill":{"duration":0.825757,"end_time":"2024-10-21T16:44:02.157626","exception":false,"start_time":"2024-10-21T16:44:01.331869","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 13 # Added +1 for the retailer\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 20\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:37.700375Z","iopub.execute_input":"2024-10-22T07:45:37.700667Z","iopub.status.idle":"2024-10-22T07:45:37.735177Z","shell.execute_reply.started":"2024-10-22T07:45:37.700637Z","shell.execute_reply":"2024-10-22T07:45:37.734091Z"},"papermill":{"duration":0.084507,"end_time":"2024-10-21T16:44:02.258847","exception":false,"start_time":"2024-10-21T16:44:02.174340","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:37.737405Z","iopub.execute_input":"2024-10-22T07:45:37.737717Z","iopub.status.idle":"2024-10-22T07:45:37.768080Z","shell.execute_reply.started":"2024-10-22T07:45:37.737685Z","shell.execute_reply":"2024-10-22T07:45:37.767177Z"},"papermill":{"duration":0.022581,"end_time":"2024-10-21T16:44:02.297614","exception":false,"start_time":"2024-10-21T16:44:02.275033","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:37.769168Z","iopub.execute_input":"2024-10-22T07:45:37.769493Z","iopub.status.idle":"2024-10-22T07:45:37.781275Z","shell.execute_reply.started":"2024-10-22T07:45:37.769462Z","shell.execute_reply":"2024-10-22T07:45:37.780393Z"},"papermill":{"duration":0.030167,"end_time":"2024-10-21T16:44:02.343766","exception":false,"start_time":"2024-10-21T16:44:02.313599","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:37.790014Z","iopub.execute_input":"2024-10-22T07:45:37.790390Z","iopub.status.idle":"2024-10-22T07:45:37.849730Z","shell.execute_reply.started":"2024-10-22T07:45:37.790347Z","shell.execute_reply":"2024-10-22T07:45:37.847960Z"},"papermill":{"duration":21.880545,"end_time":"2024-10-21T16:44:24.240278","exception":false,"start_time":"2024-10-21T16:44:02.359733","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnhancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n        super().__init__()\n        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n        self.hidden_size = hidden_size\n        \n        # Original classifiers\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        \n        # Original RL Policy networks\n        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        \n        # Original contrastive learning projection\n        self.projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim)\n        )\n        \n        # Original few-shot learning prototypes\n        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n        \n        # FIXED: Multi-head attention for enhanced feature extraction\n        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n        \n        # Rest of the architecture remains the same\n        self.expert_classifiers = nn.ModuleDict({\n            'supergroup': self._make_expert_classifier(hidden_size, num_supergroups),\n            'group': self._make_expert_classifier(hidden_size + num_supergroups, num_groups),\n            'module': self._make_expert_classifier(hidden_size + num_supergroups + num_groups, num_modules),\n            'brand': self._make_expert_classifier(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        })\n        \n        self.confidence_heads = nn.ModuleDict({\n            'supergroup': nn.Linear(hidden_size, 1),\n            'group': nn.Linear(hidden_size + num_supergroups, 1),\n            'module': nn.Linear(hidden_size + num_supergroups + num_groups, 1),\n            'brand': nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, 1)\n        })\n        \n        self.auxiliary_projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim // 2)\n        )\n\n    def _make_expert_classifier(self, input_dim, output_dim):\n        return nn.Sequential(\n            nn.Linear(input_dim, input_dim // 2),\n            nn.LayerNorm(input_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(input_dim // 2, output_dim)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n        \n        # FIXED: Correct attention mask handling\n        # Transpose hidden states to match attention input requirements [seq_len, batch_size, hidden_size]\n        hidden_states = hidden_states.transpose(0, 1)\n        \n        # Transform attention_mask for key_padding_mask\n        # attention_mask: [batch_size, seq_len] -> [batch_size, seq_len]\n        key_padding_mask = attention_mask.bool()\n        key_padding_mask = ~key_padding_mask  # Invert mask as per PyTorch convention\n        \n        # Apply multi-head attention with corrected dimensions\n        attended_output, _ = self.attention(\n            hidden_states,  # [seq_len, batch_size, hidden_size]\n            hidden_states,  # [seq_len, batch_size, hidden_size]\n            hidden_states,  # [seq_len, batch_size, hidden_size]\n            key_padding_mask=key_padding_mask  # [batch_size, seq_len]\n        )\n        \n        # Transform back to [batch_size, seq_len, hidden_size]\n        attended_output = attended_output.transpose(0, 1)\n        \n        # Pool the attended output\n        pooled_output = torch.mean(attended_output, dim=1)\n        \n        # Rest of the forward pass remains the same\n        supergroup_logits = self.supergroup_classifier(pooled_output)\n        group_input = torch.cat([pooled_output, torch.softmax(supergroup_logits, dim=1)], dim=1)\n        group_logits = self.group_classifier(group_input)\n        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n        module_logits = self.module_classifier(module_input)\n        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n        brand_logits = self.brand_classifier(brand_input)\n        \n        supergroup_policy = self.supergroup_policy(pooled_output)\n        group_policy = self.group_policy(group_input)\n        module_policy = self.module_policy(module_input)\n        brand_policy = self.brand_policy(brand_input)\n        \n        projection = self.projection(pooled_output)\n        prototype_distances = torch.cdist(pooled_output, self.prototypes)\n        few_shot_logits = -prototype_distances\n        \n        expert_supergroup = self.expert_classifiers['supergroup'](pooled_output)\n        expert_group = self.expert_classifiers['group'](group_input)\n        expert_module = self.expert_classifiers['module'](module_input)\n        expert_brand = self.expert_classifiers['brand'](brand_input)\n        \n        confidences = {\n            'supergroup': torch.sigmoid(self.confidence_heads['supergroup'](pooled_output)),\n            'group': torch.sigmoid(self.confidence_heads['group'](group_input)),\n            'module': torch.sigmoid(self.confidence_heads['module'](module_input)),\n            'brand': torch.sigmoid(self.confidence_heads['brand'](brand_input))\n        }\n        \n        aux_projection = self.auxiliary_projection(pooled_output)\n        \n        return (\n            (supergroup_logits, group_logits, module_logits, brand_logits),\n            (supergroup_policy, group_policy, module_policy, brand_policy),\n            projection,\n            few_shot_logits,\n            (expert_supergroup, expert_group, expert_module, expert_brand),\n            confidences,\n            aux_projection\n        )\n\nclass EnhancedJointAccuracyTrainer:\n    def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n        self.model = model\n        self.device = next(model.parameters()).device\n        \n        # Original optimizers\n        self.supervised_optimizer = torch.optim.NAdam(model.parameters(), lr=supervised_lr)\n        self.rl_optimizer = torch.optim.NAdam(model.parameters(), lr=rl_lr)\n        \n        # Loss parameters\n        self.criterion = nn.CrossEntropyLoss()\n        self.contrastive_temperature = contrastive_temperature\n        self.loss_weights = loss_weights or [1.0, 1.0, 1.0, 1.0]\n        \n        # Knowledge distillation temperature\n        self.kd_temperature = 2.0\n        \n        # Focal loss gamma parameters\n        self.focal_gamma = 2.0\n\n    def compute_joint_loss(self, all_outputs, true_labels, batch_size):\n        supervised_logits, policy_logits, projection, few_shot_logits, expert_logits, confidences, aux_projection = all_outputs\n        \n        # Classification losses\n        base_losses = [\n            self.criterion(logits, true_labels[f'labels{i+1}'])\n            for i, logits in enumerate(supervised_logits)\n        ]\n        \n        # Weighted sum\n        original_loss = sum(w * l for w, l in zip(self.loss_weights, base_losses))\n        \n        # Expert ensemble loss with knowledge distillation\n        expert_losses = []\n        for base_logit, expert_logit, true_label in zip(supervised_logits, expert_logits, true_labels.values()):\n            # Knowledge distillation loss\n            soft_base = F.softmax(base_logit / self.kd_temperature, dim=1)\n            soft_expert = F.softmax(expert_logit / self.kd_temperature, dim=1)\n            kd_loss = F.kl_div(\n                F.log_softmax(base_logit / self.kd_temperature, dim=1),\n                soft_expert,\n                reduction='batchmean'\n            ) * (self.kd_temperature ** 2)\n            \n            # Focal loss for hard labels\n            ce_loss = F.cross_entropy(expert_logit, true_label, reduction='none')\n            pt = torch.exp(-ce_loss)\n            focal_loss = ((1 - pt) ** self.focal_gamma) * ce_loss\n            \n            expert_losses.append(kd_loss + focal_loss.mean())\n        \n        # Confidence-weighted loss\n        confidence_loss = 0\n        for logits, conf in zip(supervised_logits, confidences.values()):\n            pred_prob = F.softmax(logits, dim=1)\n            confidence_loss += F.mse_loss(conf, torch.max(pred_prob, dim=1)[0])\n        \n        # FIXED: Main contrastive loss with proper normalization\n        proj_norm = F.normalize(projection, dim=1)\n        similarity = torch.matmul(proj_norm, proj_norm.t()) / self.contrastive_temperature\n        contrastive_labels = torch.arange(batch_size).to(self.device)\n        contrastive_loss = F.cross_entropy(similarity, contrastive_labels)\n        \n        # FIXED: Auxiliary contrastive loss with proper dimension handling\n        aux_proj_norm = F.normalize(aux_projection, dim=1)\n        # Project aux_proj_norm to the same dimension as proj_norm if needed\n        if aux_proj_norm.size(1) != proj_norm.size(1):\n            projection_layer = nn.Linear(aux_proj_norm.size(1), proj_norm.size(1)).to(self.device)\n            aux_proj_norm = projection_layer(aux_proj_norm)\n            aux_proj_norm = F.normalize(aux_proj_norm, dim=1)\n        \n        aux_similarity = torch.matmul(aux_proj_norm, proj_norm.t()) / self.contrastive_temperature\n        aux_contrastive_loss = F.cross_entropy(aux_similarity, contrastive_labels)\n        \n        # Few-shot loss\n        few_shot_loss = F.cross_entropy(few_shot_logits, true_labels['labels1'])\n        \n        # Combine all losses with weights\n        total_loss = (\n            original_loss +\n            0.5 * sum(expert_losses) +\n            0.1 * confidence_loss +\n            0.1 * contrastive_loss +\n            0.05 * aux_contrastive_loss +\n            0.1 * few_shot_loss\n        )\n        \n        return total_loss\n\n    def supervised_step(self, batch, true_labels):\n        self.supervised_optimizer.zero_grad()\n        batch_size = batch['input_ids'].size(0)\n        all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n        total_loss = self.compute_joint_loss(all_outputs, true_labels, batch_size)\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n        self.supervised_optimizer.step()\n        return total_loss.item()\n\ndef train_and_evaluate(model, train_loader, val_loader, num_epochs=10):\n    trainer = EnhancedJointAccuracyTrainer(model)\n    best_accuracy = 0\n    patience = 5\n    no_improve = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        \n        for batch_idx, batch in enumerate(train_loader):\n            batch = {k: v.to(trainer.device) for k, v in batch.items()}\n            true_labels = {\n                f'labels{i+1}': batch[f'labels{i+1}']\n                for i in range(4)\n            }\n            loss = trainer.supervised_step(batch, true_labels)\n            total_loss += loss\n            \n            if batch_idx % 100 == 0:\n                print(f\"Batch {batch_idx}, Loss: {loss:.4f}\")\n        \n        avg_loss = total_loss / len(train_loader)\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(f\"Average Training Loss: {avg_loss:.4f}\")\n        \n        # Validation\n        model.eval()\n        val_accuracies = [0, 0, 0, 0, 0]\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(trainer.device) for k, v in batch.items()}\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                supervised_logits = outputs[0]\n                \n                true_labels = {\n                    f'labels{i+1}': batch[f'labels{i+1}']\n                    for i in range(4)\n                }\n                \n                val_loss += trainer.compute_joint_loss(outputs, true_labels, batch['input_ids'].size(0)).item()\n                \n                # Calculate accuracies\n                for i, logits in enumerate(supervised_logits):\n                    preds = torch.argmax(logits, dim=1)\n                    val_accuracies[i] += (preds == batch[f'labels{i+1}']).float().mean().item()\n                \n                # Calculate joint accuracy\n                all_correct = torch.all(torch.stack([\n                    torch.argmax(logits, dim=1) == batch[f'labels{i+1}']\n                    for i, logits in enumerate(supervised_logits)\n                ]), dim=0)\n                val_accuracies[4] += all_correct.float().mean().item()\n        \n        val_accuracies = [acc / len(val_loader) for acc in val_accuracies]\n        avg_val_loss = val_loss / len(val_loader)\n        \n        print(f\"\\nValidation Results:\")\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n        print(f\"Supergroup Accuracy: {val_accuracies[0]:.4f}\")\n        print(f\"Group Accuracy: {val_accuracies[1]:.4f}\")\n        print(f\"Module Accuracy: {val_accuracies[2]:.4f}\")\n        print(f\"Brand Accuracy: {val_accuracies[3]:.4f}\")\n        print(f\"Joint Accuracy: {val_accuracies[4]:.4f}\")\n        \n        # Save best model and early stopping\n        if val_accuracies[4] > best_accuracy:\n            best_accuracy = val_accuracies[4]\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(f\"New best model saved with joint accuracy: {best_accuracy:.4f}\")\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(\"Early stopping triggered after\", patience, \"epochs without improvement\")\n                break\n\n# Usage\nmodel = EnhancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\n\n# For loading the model saved after pre-training the saved dict\n# model_path = os.path.join(model_dir, \"model.pth\")\n# model.load_state_dict(torch.load(model_path))\n\n# Assuming you have train_loader and val_loader already defined\ntrain_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:45:42.749640Z","iopub.execute_input":"2024-10-22T07:45:42.749991Z","iopub.status.idle":"2024-10-22T07:46:07.189908Z","shell.execute_reply.started":"2024-10-22T07:45:42.749958Z","shell.execute_reply":"2024-10-22T07:46:07.188985Z"},"papermill":{"duration":37444.232661,"end_time":"2024-10-22T03:08:28.489683","exception":false,"start_time":"2024-10-21T16:44:24.257022","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n\n# Create the directory if it doesn't exist\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n\n# Save the model's state dictionary\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:46:07.220648Z","iopub.execute_input":"2024-10-22T07:46:07.221003Z","iopub.status.idle":"2024-10-22T07:46:09.394362Z","shell.execute_reply.started":"2024-10-22T07:46:07.220957Z","shell.execute_reply":"2024-10-22T07:46:09.393359Z"},"papermill":{"duration":2.154582,"end_time":"2024-10-22T03:08:30.663580","exception":false,"start_time":"2024-10-22T03:08:28.508998","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:46:09.396139Z","iopub.execute_input":"2024-10-22T07:46:09.396499Z","iopub.status.idle":"2024-10-22T07:46:10.188229Z","shell.execute_reply.started":"2024-10-22T07:46:09.396465Z","shell.execute_reply":"2024-10-22T07:46:10.187376Z"},"papermill":{"duration":0.940418,"end_time":"2024-10-22T03:08:31.623742","exception":false,"start_time":"2024-10-22T03:08:30.683324","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat['description'] = df_test_feat['description'] + ' ' + df_test_feat['retailer']\ndf_test_feat.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:48:33.912385Z","iopub.execute_input":"2024-10-22T07:48:33.913122Z","iopub.status.idle":"2024-10-22T07:48:33.975394Z","shell.execute_reply.started":"2024-10-22T07:48:33.913081Z","shell.execute_reply":"2024-10-22T07:48:33.974383Z"},"papermill":{"duration":0.046924,"end_time":"2024-10-22T03:08:31.691949","exception":false,"start_time":"2024-10-22T03:08:31.645025","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, text, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"\n    Make predictions using the enhanced hierarchical classifier.\n    \n    Args:\n        model: The trained EnhancedHierarchicalClassifier model\n        tokenizer: The tokenizer used for preprocessing\n        text: Input text to classify\n        device: Device to run inference on\n        \n    Returns:\n        dict: Dictionary containing predictions and confidence scores\n    \"\"\"\n    # Ensure model is in eval mode\n    model.eval()\n    \n    # Tokenize input\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=512,  # Using standard MAX_LENGTH, adjust if needed\n        return_tensors=\"pt\"\n    ).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n        \n        # Unpack model outputs\n        logits, _, _, _, expert_logits, confidences, _ = outputs\n        \n        # Get predictions from main classifiers\n        predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n        \n        # Get confidence scores\n        confidence_scores = {\n            'supergroup': confidences['supergroup'].item(),\n            'group': confidences['group'].item(),\n            'module': confidences['module'].item(),\n            'brand': confidences['brand'].item()\n        }\n        \n        # Get probabilities\n        probabilities = [F.softmax(logit, dim=1).max(1).values.item() for logit in logits]\n        \n        # Get expert predictions\n        expert_predictions = [torch.argmax(logit, dim=1).item() for logit in expert_logits]\n        \n    # Combine results\n    result = {\n        'predictions': {\n            'supergroup': predictions[0],\n            'group': predictions[1],\n            'module': predictions[2],\n            'brand': predictions[3]\n        },\n        'expert_predictions': {\n            'supergroup': expert_predictions[0],\n            'group': expert_predictions[1],\n            'module': expert_predictions[2],\n            'brand': expert_predictions[3]\n        },\n        'confidence_scores': confidence_scores,\n        'probabilities': {\n            'supergroup': probabilities[0],\n            'group': probabilities[1],\n            'module': probabilities[2],\n            'brand': probabilities[3]\n        }\n    }\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:49:14.675080Z","iopub.execute_input":"2024-10-22T07:49:14.675505Z","iopub.status.idle":"2024-10-22T07:49:14.687303Z","shell.execute_reply.started":"2024-10-22T07:49:14.675467Z","shell.execute_reply":"2024-10-22T07:49:14.686318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reconcile_predictions(\n    model, \n    tokenizer, \n    text: str,\n    confidence_threshold: float = 0.8,\n    probability_threshold: float = 0.7,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n) -> Tuple[int, int, int, int]:\n    \"\"\"\n    End-to-end function that manages prediction reconciliation between main and expert models.\n    \n    Args:\n        model: The trained EnhancedHierarchicalClassifier model\n        tokenizer: The tokenizer used for preprocessing\n        text: Input text to classify\n        confidence_threshold: Minimum confidence score to trust main prediction\n        probability_threshold: Minimum probability score to trust main prediction\n        device: Device to run inference on\n        \n    Returns:\n        Tuple[int, int, int, int]: Final predictions for (supergroup, group, module, brand)\n    \"\"\"\n    # Get initial predictions\n    predictions = predict(model, tokenizer, text, device)\n    \n    # Initialize final predictions\n    final_predictions = []\n    \n    # Process each category\n    categories = ['supergroup', 'group', 'module', 'brand']\n    for category in categories:\n        main_pred = predictions['predictions'][category]\n        expert_pred = predictions['expert_predictions'][category]\n        confidence = predictions['confidence_scores'][category]\n        probability = predictions['probabilities'][category]\n        \n        # Decision logic\n        if main_pred == expert_pred:\n            final_pred = main_pred\n        else:\n            # Case 1: High confidence and probability in main prediction\n            if confidence >= confidence_threshold and probability >= probability_threshold:\n                final_pred = main_pred\n            \n            # Case 2: Low confidence or probability - trust expert\n            elif confidence < confidence_threshold or probability < probability_threshold:\n                final_pred = expert_pred\n            \n            # Case 3: Moderate confidence/probability - use weighted ensemble\n            else:\n                final_pred = _weighted_ensemble_decision(\n                    main_pred=main_pred,\n                    expert_pred=expert_pred,\n                    confidence=confidence,\n                    probability=probability\n                )\n        \n        final_predictions.append(final_pred)\n    \n    return tuple(final_predictions)\n\ndef _weighted_ensemble_decision(\n    main_pred: int,\n    expert_pred: int,\n    confidence: float,\n    probability: float\n) -> int:\n    \"\"\"\n    Make a weighted decision between main and expert predictions.\n    \n    Args:\n        main_pred: Prediction from main classifier\n        expert_pred: Prediction from expert classifier\n        confidence: Confidence score from main classifier\n        probability: Probability score from main classifier\n        \n    Returns:\n        int: Final prediction\n    \"\"\"\n    main_weight = (confidence + probability) / 2\n    return main_pred if main_weight >= 0.5 else expert_pred\n\n# Single prediction\ntext = \"Sample product description\" + \"retailer name\"\nsupergroup, group, module, brand = reconcile_predictions(model, tokenizer, text)\nprint(supergroup, group, module, brand)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:49:17.095223Z","iopub.execute_input":"2024-10-22T07:49:17.095618Z","iopub.status.idle":"2024-10-22T07:49:17.136913Z","shell.execute_reply.started":"2024-10-22T07:49:17.095580Z","shell.execute_reply":"2024-10-22T07:49:17.136073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test_feat = df_test_feat[:50]","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:39:39.669170Z","iopub.execute_input":"2024-10-22T07:39:39.670108Z","iopub.status.idle":"2024-10-22T07:39:39.674154Z","shell.execute_reply.started":"2024-10-22T07:39:39.670068Z","shell.execute_reply":"2024-10-22T07:39:39.673284Z"},"papermill":{"duration":0.026937,"end_time":"2024-10-22T03:08:31.829731","exception":false,"start_time":"2024-10-22T03:08:31.802794","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(0, len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    with torch.no_grad():\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n            predictions = reconcile_predictions(model, tokenizer, df_test_feat.iloc[i].description)\n            \n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n\n        try:\n            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n        except ValueError as e:\n            print(f\"Error in supergroups: {e}\")\n            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n        try:\n            groups_names = group_encoder.inverse_transform(groups_list)\n        except ValueError as e:\n            print(f\"Error in groups: {e}\")\n            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n        try:\n            modules_names = module_encoder.inverse_transform(modules_list)\n        except ValueError as e:\n            print(f\"Error in modules: {e}\")\n            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n        try:\n            brands_names = brand_encoder.inverse_transform(brands_list)\n        except ValueError as e:\n            print(f\"Error in brands: {e}\")\n            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n        # Create a DataFrame with predictions\n        predictions_df = pd.DataFrame({\n            'indoml_id': indoml_id_list,\n            'supergroup': supergroups_names,\n            'group': groups_names,\n            'module': modules_names,\n            'brand': brands_names\n        })\n        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n        print(\"predictions.predict saved\")\nmake_test_pred_and_save(df_test_feat)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T07:49:42.878365Z","iopub.execute_input":"2024-10-22T07:49:42.878756Z","iopub.status.idle":"2024-10-22T07:49:55.593910Z","shell.execute_reply.started":"2024-10-22T07:49:42.878720Z","shell.execute_reply":"2024-10-22T07:49:55.592485Z"},"papermill":{"duration":1760.492917,"end_time":"2024-10-22T03:37:52.342903","exception":false,"start_time":"2024-10-22T03:08:31.849986","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.033131,"end_time":"2024-10-22T03:37:52.409939","exception":false,"start_time":"2024-10-22T03:37:52.376808","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.033122,"end_time":"2024-10-22T03:37:52.476348","exception":false,"start_time":"2024-10-22T03:37:52.443226","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}