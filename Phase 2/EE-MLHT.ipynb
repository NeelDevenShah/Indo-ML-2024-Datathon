{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd92afb8",
   "metadata": {
    "papermill": {
     "duration": 0.012274,
     "end_time": "2024-10-28T10:27:24.063481",
     "exception": false,
     "start_time": "2024-10-28T10:27:24.051207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1>Do not use the weights from v4 of the notebook, i.e. v3 version of weights, trained on retailer thing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4c75df",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:24.088856Z",
     "iopub.status.busy": "2024-10-28T10:27:24.088475Z",
     "iopub.status.idle": "2024-10-28T10:27:35.041890Z",
     "shell.execute_reply": "2024-10-28T10:27:35.040877Z"
    },
    "papermill": {
     "duration": 10.968345,
     "end_time": "2024-10-28T10:27:35.044223",
     "exception": false,
     "start_time": "2024-10-28T10:27:24.075878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "df_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\n",
    "df_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d107cf82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:35.068653Z",
     "iopub.status.busy": "2024-10-28T10:27:35.068307Z",
     "iopub.status.idle": "2024-10-28T10:27:35.608274Z",
     "shell.execute_reply": "2024-10-28T10:27:35.607206Z"
    },
    "papermill": {
     "duration": 0.554419,
     "end_time": "2024-10-28T10:27:35.610350",
     "exception": false,
     "start_time": "2024-10-28T10:27:35.055931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "huggingface_token = \"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\"\n",
    "login(token=huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce07fd97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:35.634340Z",
     "iopub.status.busy": "2024-10-28T10:27:35.634052Z",
     "iopub.status.idle": "2024-10-28T10:27:35.781483Z",
     "shell.execute_reply": "2024-10-28T10:27:35.780655Z"
    },
    "papermill": {
     "duration": 0.161856,
     "end_time": "2024-10-28T10:27:35.783672",
     "exception": false,
     "start_time": "2024-10-28T10:27:35.621816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_features,df_labels,on=\"indoml_id\")\n",
    "# df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80208ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:35.807961Z",
     "iopub.status.busy": "2024-10-28T10:27:35.807638Z",
     "iopub.status.idle": "2024-10-28T10:27:36.593351Z",
     "shell.execute_reply": "2024-10-28T10:27:36.592562Z"
    },
    "papermill": {
     "duration": 0.800296,
     "end_time": "2024-10-28T10:27:36.595634",
     "exception": false,
     "start_time": "2024-10-28T10:27:35.795338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "group_encoder = LabelEncoder()\n",
    "supergroup_encoder = LabelEncoder()\n",
    "module_encoder = LabelEncoder()\n",
    "brand_encoder = LabelEncoder()\n",
    "# Fit and transform each column\n",
    "df['group'] = group_encoder.fit_transform(df['group'])\n",
    "df['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\n",
    "df['module'] = module_encoder.fit_transform(df['module'])\n",
    "df['brand'] = brand_encoder.fit_transform(df['brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ca00d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:36.620000Z",
     "iopub.status.busy": "2024-10-28T10:27:36.619713Z",
     "iopub.status.idle": "2024-10-28T10:27:38.407781Z",
     "shell.execute_reply": "2024-10-28T10:27:38.406768Z"
    },
    "papermill": {
     "duration": 1.802892,
     "end_time": "2024-10-28T10:27:38.410231",
     "exception": false,
     "start_time": "2024-10-28T10:27:36.607339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "# from transformers import XLNetTokenizer, XLNetModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d1dbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:38.435293Z",
     "iopub.status.busy": "2024-10-28T10:27:38.434586Z",
     "iopub.status.idle": "2024-10-28T10:27:39.255992Z",
     "shell.execute_reply": "2024-10-28T10:27:39.254737Z"
    },
    "papermill": {
     "duration": 0.837167,
     "end_time": "2024-10-28T10:27:39.259479",
     "exception": false,
     "start_time": "2024-10-28T10:27:38.422312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For using the new one\n",
    "# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "# For loading the saved one\n",
    "model_dir = \"/kaggle/input/new-transformer-experiment-12-emb-xlm-roberta-tmp/New_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15a8831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:39.288933Z",
     "iopub.status.busy": "2024-10-28T10:27:39.288499Z",
     "iopub.status.idle": "2024-10-28T10:27:39.360536Z",
     "shell.execute_reply": "2024-10-28T10:27:39.359556Z"
    },
    "papermill": {
     "duration": 0.089513,
     "end_time": "2024-10-28T10:27:39.362777",
     "exception": false,
     "start_time": "2024-10-28T10:27:39.273264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 12\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 22\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device=DEVICE\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "PATIENCE_LR = 3  # Reduce LR on plateau patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c274db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:39.394593Z",
     "iopub.status.busy": "2024-10-28T10:27:39.393724Z",
     "iopub.status.idle": "2024-10-28T10:27:39.399478Z",
     "shell.execute_reply": "2024-10-28T10:27:39.398586Z"
    },
    "papermill": {
     "duration": 0.027428,
     "end_time": "2024-10-28T10:27:39.401872",
     "exception": false,
     "start_time": "2024-10-28T10:27:39.374444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796c102e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:39.430228Z",
     "iopub.status.busy": "2024-10-28T10:27:39.429824Z",
     "iopub.status.idle": "2024-10-28T10:27:39.440947Z",
     "shell.execute_reply": "2024-10-28T10:27:39.439967Z"
    },
    "papermill": {
     "duration": 0.02523,
     "end_time": "2024-10-28T10:27:39.442904",
     "exception": false,
     "start_time": "2024-10-28T10:27:39.417674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, texts, labels1, labels2, labels3, labels4):\n",
    "        self.texts = texts\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "        self.labels3 = labels3\n",
    "        self.labels4 = labels4\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n",
    "        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n",
    "        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n",
    "        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n",
    "        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n",
    "        return item\n",
    "        \n",
    "def compute_accuracy(preds, labels):\n",
    "    # Convert each tensor in the list to numpy arrays\n",
    "    preds_np = [p.cpu().numpy() for p in preds]\n",
    "    labels_np = [l.cpu().numpy() for l in labels]\n",
    "    # Individual accuracies for each of the 4 labels\n",
    "    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n",
    "    # Overall accuracy where all 4 labels match\n",
    "    overall_accuracy = accuracy_score(\n",
    "        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n",
    "        np.ones(len(labels_np[0]))\n",
    "    )\n",
    "    # Return the 5 accuracies (4 individual, 1 overall)\n",
    "    return accuracies + [overall_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306f2cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:39.467547Z",
     "iopub.status.busy": "2024-10-28T10:27:39.466624Z",
     "iopub.status.idle": "2024-10-28T10:27:59.455807Z",
     "shell.execute_reply": "2024-10-28T10:27:59.454999Z"
    },
    "papermill": {
     "duration": 20.003885,
     "end_time": "2024-10-28T10:27:59.458199",
     "exception": false,
     "start_time": "2024-10-28T10:27:39.454314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n",
    "    df['description'], \n",
    "    df['supergroup'], \n",
    "    df['group'], \n",
    "    df['module'], \n",
    "    df['brand'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "train_dataset = ProductDataset(\n",
    "    train_texts.tolist(), \n",
    "    train_labels1.tolist(), \n",
    "    train_labels2.tolist(), \n",
    "    train_labels3.tolist(), \n",
    "    train_labels4.tolist()\n",
    ")\n",
    "val_dataset = ProductDataset(\n",
    "    val_texts.tolist(), \n",
    "    val_labels1.tolist(), \n",
    "    val_labels2.tolist(), \n",
    "    val_labels3.tolist(), \n",
    "    val_labels4.tolist()\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90338e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:59.482688Z",
     "iopub.status.busy": "2024-10-28T10:27:59.482349Z",
     "iopub.status.idle": "2024-10-28T10:27:59.504051Z",
     "shell.execute_reply": "2024-10-28T10:27:59.503314Z"
    },
    "papermill": {
     "duration": 0.035965,
     "end_time": "2024-10-28T10:27:59.505924",
     "exception": false,
     "start_time": "2024-10-28T10:27:59.469959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from transformers import DebertaModel, DebertaTokenizer\n",
    "from transformers import XLNetTokenizer, XLMRobertaModel\n",
    "from torch.distributions import Categorical\n",
    "import os\n",
    "class AdvancedHierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n",
    "        super().__init__()\n",
    "        # For loading the new model\n",
    "        # self.model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.hidden_size = hidden_size\n",
    "        # Classifiers for each hierarchy level\n",
    "        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n",
    "        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n",
    "        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n",
    "        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n",
    "        # RL Policy networks for each level\n",
    "        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n",
    "        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n",
    "        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n",
    "        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n",
    "        # Contrastive learning projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, projection_dim)\n",
    "        )\n",
    "        # Few-shot learning prototypes\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
    "        # Supervised classification logits\n",
    "        supergroup_logits = self.supergroup_classifier(hidden_states)\n",
    "        group_input = torch.cat([hidden_states, torch.softmax(supergroup_logits, dim=1)], dim=1)\n",
    "        group_logits = self.group_classifier(group_input)\n",
    "        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n",
    "        module_logits = self.module_classifier(module_input)\n",
    "        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n",
    "        brand_logits = self.brand_classifier(brand_input)\n",
    "        # RL policy logits\n",
    "        supergroup_policy = self.supergroup_policy(hidden_states)\n",
    "        group_policy = self.group_policy(group_input)\n",
    "        module_policy = self.module_policy(module_input)\n",
    "        brand_policy = self.brand_policy(brand_input)\n",
    "        # Contrastive learning projection\n",
    "        projection = self.projection(hidden_states)\n",
    "        # Few-shot learning\n",
    "        prototype_distances = torch.cdist(hidden_states, self.prototypes)\n",
    "        few_shot_logits = -prototype_distances  # Negative distance as logits\n",
    "        return (supergroup_logits, group_logits, module_logits, brand_logits), \\\n",
    "               (supergroup_policy, group_policy, module_policy, brand_policy), \\\n",
    "               projection, few_shot_logits\n",
    "    def sample_actions(self, policies):\n",
    "        return [Categorical(logits=policy).sample() for policy in policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd610007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:59.529770Z",
     "iopub.status.busy": "2024-10-28T10:27:59.529464Z",
     "iopub.status.idle": "2024-10-28T10:27:59.547522Z",
     "shell.execute_reply": "2024-10-28T10:27:59.546637Z"
    },
    "papermill": {
     "duration": 0.032047,
     "end_time": "2024-10-28T10:27:59.549396",
     "exception": false,
     "start_time": "2024-10-28T10:27:59.517349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class JointAccuracyTrainer:\n",
    "    def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.supervised_optimizer = torch.optim.NAdam(model.parameters(), lr=supervised_lr)\n",
    "        self.rl_optimizer = torch.optim.NAdam(model.parameters(), lr=rl_lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.contrastive_temperature = contrastive_temperature\n",
    "        if loss_weights is None:\n",
    "            self.loss_weights = [1.0, 1.0, 1.0, 1.0]\n",
    "        else:\n",
    "            self.loss_weights = loss_weights\n",
    "\n",
    "    def compute_joint_loss(self, all_outputs, true_labels):\n",
    "        supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n",
    "        logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n",
    "        loss_supergroup = self.criterion(logits_supergroup, true_labels['supergroup'])\n",
    "        loss_group1 = self.criterion(logits_group1, true_labels['group1'])\n",
    "        loss_group2 = self.criterion(logits_group2, true_labels['group2'])\n",
    "        loss_group3 = self.criterion(logits_group3, true_labels['group3'])\n",
    "        total_loss = (\n",
    "            self.loss_weights[0] * loss_supergroup + \n",
    "            self.loss_weights[1] * loss_group1 + \n",
    "            self.loss_weights[2] * loss_group2 + \n",
    "            self.loss_weights[3] * loss_group3\n",
    "        )\n",
    "        return total_loss\n",
    "\n",
    "    def supervised_step(self, batch, true_labels):\n",
    "        self.supervised_optimizer.zero_grad()\n",
    "        all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n",
    "        total_loss = self.compute_joint_loss(all_outputs, true_labels)\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "        self.supervised_optimizer.step()\n",
    "        return total_loss.item()\n",
    "\n",
    "    def validation_step(self, batch, true_labels):\n",
    "        with torch.no_grad():\n",
    "            all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n",
    "            supervised_logits, _, _, _ = all_outputs\n",
    "            logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n",
    "            preds_supergroup = torch.argmax(logits_supergroup, dim=-1)\n",
    "            preds_group1 = torch.argmax(logits_group1, dim=-1)\n",
    "            preds_group2 = torch.argmax(logits_group2, dim=-1)\n",
    "            preds_group3 = torch.argmax(logits_group3, dim=-1)\n",
    "            supergroup_acc = (preds_supergroup == true_labels['supergroup']).float().mean().item()\n",
    "            group1_acc = (preds_group1 == true_labels['group1']).float().mean().item()\n",
    "            group2_acc = (preds_group2 == true_labels['group2']).float().mean().item()\n",
    "            group3_acc = (preds_group3 == true_labels['group3']).float().mean().item()\n",
    "            item_acc = ((preds_supergroup == true_labels['supergroup']) &\n",
    "                        (preds_group1 == true_labels['group1']) &\n",
    "                        (preds_group2 == true_labels['group2']) &\n",
    "                        (preds_group3 == true_labels['group3'])).float().mean().item()\n",
    "        return supergroup_acc, group1_acc, group2_acc, group3_acc, item_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee91351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:27:59.572991Z",
     "iopub.status.busy": "2024-10-28T10:27:59.572682Z",
     "iopub.status.idle": "2024-10-28T10:28:13.117588Z",
     "shell.execute_reply": "2024-10-28T10:28:13.116695Z"
    },
    "papermill": {
     "duration": 13.558958,
     "end_time": "2024-10-28T10:28:13.119666",
     "exception": false,
     "start_time": "2024-10-28T10:27:59.560708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b254c13a135947089958ec76dc1c9e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d6b50821204d62a05717e476e1ab62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2259319365.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "model = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\n",
    "model_path = os.path.join(model_dir, \"model.pth\")\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16147474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:28:13.145802Z",
     "iopub.status.busy": "2024-10-28T10:28:13.145153Z",
     "iopub.status.idle": "2024-10-28T10:28:17.935967Z",
     "shell.execute_reply": "2024-10-28T10:28:17.934836Z"
    },
    "papermill": {
     "duration": 4.805824,
     "end_time": "2024-10-28T10:28:17.938152",
     "exception": false,
     "start_time": "2024-10-28T10:28:13.132328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         0.86%       6.798ms         1.47%      11.713ms      76.557us      43.970ms        14.55%      43.970ms     287.383us           153    263867.311  \n",
      "                                            aten::addmm         4.21%      33.458ms        11.58%      92.068ms       1.109ms      23.665ms         7.83%      23.665ms     285.123us            83    132941.656  \n",
      "                                              aten::add         0.40%       3.167ms         1.91%      15.152ms     445.643us     353.212us         0.12%     353.212us      10.389us            34        14.913  \n",
      "                                              aten::mul         1.52%      12.111ms         3.54%      28.150ms     121.862us     211.679us         0.07%     211.679us       0.916us           231         7.173  \n",
      "                    Optimizer.zero_grad#NAdam.zero_grad         0.03%     211.300us         0.03%     211.300us     211.300us       0.000us         0.00%       0.000us       0.000us             1            --  \n",
      "                                            aten::slice         0.57%       4.502ms         0.57%       4.560ms     116.932us       0.000us         0.00%       0.000us       0.000us            39            --  \n",
      "                                       aten::as_strided         0.12%     934.398us         0.12%     934.398us       1.017us       0.000us         0.00%       0.000us       0.000us           919            --  \n",
      "                                           aten::expand         0.01%      97.197us         0.02%     126.126us       7.883us       0.000us         0.00%       0.000us       0.000us            16            --  \n",
      "                                               aten::ne         0.89%       7.053ms         3.62%      28.783ms      28.783ms       3.424us         0.00%       3.424us       3.424us             1            --  \n",
      "                                       cudaLaunchKernel        59.22%     470.645ms        59.22%     470.645ms     364.276us       0.000us         0.00%       0.000us       0.000us          1292            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 794.791ms\n",
      "Self CUDA time total: 302.283ms\n",
      "\n",
      "GFLOPs during training\n",
      "396.831053338\n"
     ]
    }
   ],
   "source": [
    "# Profile on a single sample before starting the full training\n",
    "model.train()\n",
    "sample_batch = next(iter(train_loader))  # Get one sample batch\n",
    "sample_batch = {k: v.to(device) for k, v in sample_batch.items()}\n",
    "true_labels = {\n",
    "    'supergroup': sample_batch['labels1'],\n",
    "    'group1': sample_batch['labels2'],\n",
    "    'group2': sample_batch['labels3'],\n",
    "    'group3': sample_batch['labels4']\n",
    "}\n",
    "trainer = JointAccuracyTrainer(model)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "    _ = trainer.supervised_step(sample_batch, true_labels)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n",
    "print(\"GFLOPs during training\") #GigaFLOPs\n",
    "print(sum(k.flops for k in prof.key_averages())/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ebaed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:28:17.964632Z",
     "iopub.status.busy": "2024-10-28T10:28:17.963870Z",
     "iopub.status.idle": "2024-10-28T17:20:30.282807Z",
     "shell.execute_reply": "2024-10-28T17:20:30.281866Z"
    },
    "papermill": {
     "duration": 24732.348613,
     "end_time": "2024-10-28T17:20:30.299128",
     "exception": false,
     "start_time": "2024-10-28T10:28:17.950515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with item accuracy: 0.7836\n",
      "Epoch 1/22 - Train Loss: 0.0245, Val Accuracies - Supergroup: 0.8859, Group1: 0.8587, Group2: 0.8469, Group3: 0.8735, Item Accuracy: 0.7836\n",
      "New best model saved with item accuracy: 0.7841\n",
      "Epoch 2/22 - Train Loss: 0.0245, Val Accuracies - Supergroup: 0.8864, Group1: 0.8594, Group2: 0.8479, Group3: 0.8741, Item Accuracy: 0.7841\n",
      "New best model saved with item accuracy: 0.7842\n",
      "Epoch 3/22 - Train Loss: 0.0242, Val Accuracies - Supergroup: 0.8858, Group1: 0.8586, Group2: 0.8470, Group3: 0.8745, Item Accuracy: 0.7842\n",
      "New best model saved with item accuracy: 0.7844\n",
      "Epoch 4/22 - Train Loss: 0.0235, Val Accuracies - Supergroup: 0.8861, Group1: 0.8594, Group2: 0.8473, Group3: 0.8748, Item Accuracy: 0.7844\n",
      "Epoch 5/22 - Train Loss: 0.0246, Val Accuracies - Supergroup: 0.8857, Group1: 0.8584, Group2: 0.8473, Group3: 0.8745, Item Accuracy: 0.7840\n",
      "Epoch 6/22 - Train Loss: 0.0233, Val Accuracies - Supergroup: 0.8862, Group1: 0.8582, Group2: 0.8465, Group3: 0.8739, Item Accuracy: 0.7837\n",
      "Epoch 7/22 - Train Loss: 0.0225, Val Accuracies - Supergroup: 0.8868, Group1: 0.8592, Group2: 0.8467, Group3: 0.8726, Item Accuracy: 0.7833\n",
      "Epoch 8/22 - Train Loss: 0.0220, Val Accuracies - Supergroup: 0.8866, Group1: 0.8592, Group2: 0.8474, Group3: 0.8743, Item Accuracy: 0.7841\n",
      "Epoch 9/22 - Train Loss: 0.0221, Val Accuracies - Supergroup: 0.8859, Group1: 0.8579, Group2: 0.8465, Group3: 0.8735, Item Accuracy: 0.7838\n",
      "New best model saved with item accuracy: 0.7844\n",
      "Epoch 10/22 - Train Loss: 0.0221, Val Accuracies - Supergroup: 0.8872, Group1: 0.8596, Group2: 0.8478, Group3: 0.8743, Item Accuracy: 0.7844\n",
      "Epoch 11/22 - Train Loss: 0.0231, Val Accuracies - Supergroup: 0.8865, Group1: 0.8588, Group2: 0.8470, Group3: 0.8732, Item Accuracy: 0.7835\n",
      "Epoch 12/22 - Train Loss: 0.0229, Val Accuracies - Supergroup: 0.8866, Group1: 0.8587, Group2: 0.8468, Group3: 0.8749, Item Accuracy: 0.7839\n",
      "Epoch 13/22 - Train Loss: 0.0234, Val Accuracies - Supergroup: 0.8860, Group1: 0.8585, Group2: 0.8466, Group3: 0.8734, Item Accuracy: 0.7826\n",
      "Epoch 14/22 - Train Loss: 0.0222, Val Accuracies - Supergroup: 0.8859, Group1: 0.8581, Group2: 0.8462, Group3: 0.8740, Item Accuracy: 0.7828\n",
      "Epoch 15/22 - Train Loss: 0.0218, Val Accuracies - Supergroup: 0.8858, Group1: 0.8584, Group2: 0.8472, Group3: 0.8745, Item Accuracy: 0.7839\n",
      "New best model saved with item accuracy: 0.7846\n",
      "Epoch 16/22 - Train Loss: 0.0215, Val Accuracies - Supergroup: 0.8871, Group1: 0.8599, Group2: 0.8480, Group3: 0.8744, Item Accuracy: 0.7846\n",
      "Epoch 17/22 - Train Loss: 0.0230, Val Accuracies - Supergroup: 0.8862, Group1: 0.8588, Group2: 0.8473, Group3: 0.8745, Item Accuracy: 0.7841\n",
      "Epoch 18/22 - Train Loss: 0.0221, Val Accuracies - Supergroup: 0.8857, Group1: 0.8590, Group2: 0.8473, Group3: 0.8748, Item Accuracy: 0.7841\n",
      "Epoch 19/22 - Train Loss: 0.0221, Val Accuracies - Supergroup: 0.8866, Group1: 0.8589, Group2: 0.8473, Group3: 0.8740, Item Accuracy: 0.7840\n",
      "Epoch 20/22 - Train Loss: 0.0227, Val Accuracies - Supergroup: 0.8865, Group1: 0.8589, Group2: 0.8473, Group3: 0.8750, Item Accuracy: 0.7844\n",
      "New best model saved with item accuracy: 0.7851\n",
      "Epoch 21/22 - Train Loss: 0.0238, Val Accuracies - Supergroup: 0.8874, Group1: 0.8601, Group2: 0.8483, Group3: 0.8748, Item Accuracy: 0.7851\n",
      "Epoch 22/22 - Train Loss: 0.0212, Val Accuracies - Supergroup: 0.8862, Group1: 0.8588, Group2: 0.8472, Group3: 0.8734, Item Accuracy: 0.7834\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=10):\n",
    "    trainer = JointAccuracyTrainer(model)\n",
    "    best_item_accuracy = 0.0  # Track best item accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_sup_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            true_labels = {\n",
    "                'supergroup': batch['labels1'],\n",
    "                'group1': batch['labels2'],\n",
    "                'group2': batch['labels3'],\n",
    "                'group3': batch['labels4']\n",
    "            }\n",
    "            sup_loss = trainer.supervised_step(batch, true_labels)\n",
    "            total_sup_loss += sup_loss\n",
    "\n",
    "        model.eval()\n",
    "        val_accuracies = [0, 0, 0, 0, 0]\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            true_labels = {\n",
    "                'supergroup': batch['labels1'],\n",
    "                'group1': batch['labels2'],\n",
    "                'group2': batch['labels3'],\n",
    "                'group3': batch['labels4']\n",
    "            }\n",
    "            accs = trainer.validation_step(batch, true_labels)\n",
    "            val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n",
    "\n",
    "        val_accuracies = [x / len(val_loader) for x in val_accuracies]\n",
    "        item_accuracy = val_accuracies[4]  # Current epoch's item accuracy\n",
    "        \n",
    "        # Check for best item accuracy and save model if improved\n",
    "        if item_accuracy > best_item_accuracy:\n",
    "            best_item_accuracy = item_accuracy\n",
    "            model_save_path = os.path.join(\"best_model.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"New best model saved with item accuracy: {best_item_accuracy:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, Group1: {val_accuracies[1]:.4f}, \"\n",
    "              f\"Group2: {val_accuracies[2]:.4f}, Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}\")\n",
    "\n",
    "train_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57a286b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:30.328849Z",
     "iopub.status.busy": "2024-10-28T17:20:30.328486Z",
     "iopub.status.idle": "2024-10-28T17:20:32.525781Z",
     "shell.execute_reply": "2024-10-28T17:20:32.524766Z"
    },
    "papermill": {
     "duration": 2.214587,
     "end_time": "2024-10-28T17:20:32.527837",
     "exception": false,
     "start_time": "2024-10-28T17:20:30.313250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to New_model/model.pth\n",
      "Tokenizer saved to New_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define the directory to save the model and tokenizer\n",
    "save_directory = \"New_model\"\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "# Save the model's state dictionary\n",
    "model_save_path = os.path.join(save_directory, \"model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db1c15a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:32.556975Z",
     "iopub.status.busy": "2024-10-28T17:20:32.556647Z",
     "iopub.status.idle": "2024-10-28T17:20:33.435631Z",
     "shell.execute_reply": "2024-10-28T17:20:33.434843Z"
    },
    "papermill": {
     "duration": 0.895985,
     "end_time": "2024-10-28T17:20:33.437784",
     "exception": false,
     "start_time": "2024-10-28T17:20:32.541799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/580111615.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(\"best_model.pth\")))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(os.path.join(\"best_model.pth\")))\n",
    "except:\n",
    "    print(\"Nothing to worry about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fd3224c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:33.467630Z",
     "iopub.status.busy": "2024-10-28T17:20:33.466899Z",
     "iopub.status.idle": "2024-10-28T17:20:34.360135Z",
     "shell.execute_reply": "2024-10-28T17:20:34.359325Z"
    },
    "papermill": {
     "duration": 0.910499,
     "end_time": "2024-10-28T17:20:34.362415",
     "exception": false,
     "start_time": "2024-10-28T17:20:33.451916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03bde62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:34.392791Z",
     "iopub.status.busy": "2024-10-28T17:20:34.391912Z",
     "iopub.status.idle": "2024-10-28T17:20:34.406631Z",
     "shell.execute_reply": "2024-10-28T17:20:34.405751Z"
    },
    "papermill": {
     "duration": 0.032133,
     "end_time": "2024-10-28T17:20:34.408656",
     "exception": false,
     "start_time": "2024-10-28T17:20:34.376523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indoml_id</th>\n",
       "      <th>description</th>\n",
       "      <th>retailer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14 in hybrid blade</td>\n",
       "      <td>wilko</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2 pk vent stick a fres</td>\n",
       "      <td>noshify</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4 tyrefix 450 ml</td>\n",
       "      <td>noshify</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 x 4 tyrefix 450 ml</td>\n",
       "      <td>noshify</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5 l adbluescr diesel vehicles</td>\n",
       "      <td>noshify</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   indoml_id                    description retailer  price\n",
       "0          0             14 in hybrid blade    wilko   4.50\n",
       "1          1         2 pk vent stick a fres  noshify   0.69\n",
       "2          2               4 tyrefix 450 ml  noshify   2.99\n",
       "3          3           4 x 4 tyrefix 450 ml  noshify   2.99\n",
       "4          4  5 l adbluescr diesel vehicles  noshify   4.99"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04f690f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:34.439105Z",
     "iopub.status.busy": "2024-10-28T17:20:34.438531Z",
     "iopub.status.idle": "2024-10-28T17:20:34.723285Z",
     "shell.execute_reply": "2024-10-28T17:20:34.722307Z"
    },
    "papermill": {
     "duration": 0.302279,
     "end_time": "2024-10-28T17:20:34.725475",
     "exception": false,
     "start_time": "2024-10-28T17:20:34.423196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time :0.193528413772583\n",
      "GFLOPs during testing\n",
      "2.087282096\n",
      "Supergroup: 6, Group: 44, Module: 89, Brand: 4078\n"
     ]
    }
   ],
   "source": [
    "def predict(model, tokenizer, text):\n",
    "    start = time.time()\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n",
    "        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n",
    "        \n",
    "    print(\"Inference time :\"+str(time.time()-start))\n",
    "    #print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n",
    "    print(\"GFLOPs during testing\") #GigaFLOPs\n",
    "    print(sum(k.flops for k in prof.key_averages())/1e9)\n",
    "        \n",
    "    return predictions\n",
    "# Example prediction\n",
    "sample_text = \"14 in hybrid blade\"\n",
    "predictions = predict(model, tokenizer, sample_text)\n",
    "print(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4391d5b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:34.756178Z",
     "iopub.status.busy": "2024-10-28T17:20:34.755836Z",
     "iopub.status.idle": "2024-10-28T17:20:34.759843Z",
     "shell.execute_reply": "2024-10-28T17:20:34.758979Z"
    },
    "papermill": {
     "duration": 0.021755,
     "end_time": "2024-10-28T17:20:34.761936",
     "exception": false,
     "start_time": "2024-10-28T17:20:34.740181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test_feat = df_test_feat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fcc1eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:20:34.791721Z",
     "iopub.status.busy": "2024-10-28T17:20:34.791445Z",
     "iopub.status.idle": "2024-10-28T17:54:46.891772Z",
     "shell.execute_reply": "2024-10-28T17:54:46.890833Z"
    },
    "papermill": {
     "duration": 2052.117923,
     "end_time": "2024-10-28T17:54:46.894096",
     "exception": false,
     "start_time": "2024-10-28T17:20:34.776173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 of 184663\n",
      "Processing 1000 of 184663\n",
      "Processing 2000 of 184663\n",
      "Processing 3000 of 184663\n",
      "Processing 4000 of 184663\n",
      "Processing 5000 of 184663\n",
      "Processing 6000 of 184663\n",
      "Processing 7000 of 184663\n",
      "Processing 8000 of 184663\n",
      "Processing 9000 of 184663\n",
      "Processing 10000 of 184663\n",
      "Processing 11000 of 184663\n",
      "Processing 12000 of 184663\n",
      "Processing 13000 of 184663\n",
      "Processing 14000 of 184663\n",
      "Processing 15000 of 184663\n",
      "Processing 16000 of 184663\n",
      "Processing 17000 of 184663\n",
      "Processing 18000 of 184663\n",
      "Processing 19000 of 184663\n",
      "Processing 20000 of 184663\n",
      "Processing 21000 of 184663\n",
      "Processing 22000 of 184663\n",
      "Processing 23000 of 184663\n",
      "Processing 24000 of 184663\n",
      "Processing 25000 of 184663\n",
      "Processing 26000 of 184663\n",
      "Processing 27000 of 184663\n",
      "Processing 28000 of 184663\n",
      "Processing 29000 of 184663\n",
      "Processing 30000 of 184663\n",
      "Processing 31000 of 184663\n",
      "Processing 32000 of 184663\n",
      "Processing 33000 of 184663\n",
      "Processing 34000 of 184663\n",
      "Processing 35000 of 184663\n",
      "Processing 36000 of 184663\n",
      "Processing 37000 of 184663\n",
      "Processing 38000 of 184663\n",
      "Processing 39000 of 184663\n",
      "Processing 40000 of 184663\n",
      "Processing 41000 of 184663\n",
      "Processing 42000 of 184663\n",
      "Processing 43000 of 184663\n",
      "Processing 44000 of 184663\n",
      "Processing 45000 of 184663\n",
      "Processing 46000 of 184663\n",
      "Processing 47000 of 184663\n",
      "Processing 48000 of 184663\n",
      "Processing 49000 of 184663\n",
      "Processing 50000 of 184663\n",
      "Processing 51000 of 184663\n",
      "Processing 52000 of 184663\n",
      "Processing 53000 of 184663\n",
      "Processing 54000 of 184663\n",
      "Processing 55000 of 184663\n",
      "Processing 56000 of 184663\n",
      "Processing 57000 of 184663\n",
      "Processing 58000 of 184663\n",
      "Processing 59000 of 184663\n",
      "Processing 60000 of 184663\n",
      "Processing 61000 of 184663\n",
      "Processing 62000 of 184663\n",
      "Processing 63000 of 184663\n",
      "Processing 64000 of 184663\n",
      "Processing 65000 of 184663\n",
      "Processing 66000 of 184663\n",
      "Processing 67000 of 184663\n",
      "Processing 68000 of 184663\n",
      "Processing 69000 of 184663\n",
      "Processing 70000 of 184663\n",
      "Processing 71000 of 184663\n",
      "Processing 72000 of 184663\n",
      "Processing 73000 of 184663\n",
      "Processing 74000 of 184663\n",
      "Processing 75000 of 184663\n",
      "Processing 76000 of 184663\n",
      "Processing 77000 of 184663\n",
      "Processing 78000 of 184663\n",
      "Processing 79000 of 184663\n",
      "Processing 80000 of 184663\n",
      "Processing 81000 of 184663\n",
      "Processing 82000 of 184663\n",
      "Processing 83000 of 184663\n",
      "Processing 84000 of 184663\n",
      "Processing 85000 of 184663\n",
      "Processing 86000 of 184663\n",
      "Processing 87000 of 184663\n",
      "Processing 88000 of 184663\n",
      "Processing 89000 of 184663\n",
      "Processing 90000 of 184663\n",
      "Processing 91000 of 184663\n",
      "Processing 92000 of 184663\n",
      "Processing 93000 of 184663\n",
      "Processing 94000 of 184663\n",
      "Processing 95000 of 184663\n",
      "Processing 96000 of 184663\n",
      "Processing 97000 of 184663\n",
      "Processing 98000 of 184663\n",
      "Processing 99000 of 184663\n",
      "Processing 100000 of 184663\n",
      "Processing 101000 of 184663\n",
      "Processing 102000 of 184663\n",
      "Processing 103000 of 184663\n",
      "Processing 104000 of 184663\n",
      "Processing 105000 of 184663\n",
      "Processing 106000 of 184663\n",
      "Processing 107000 of 184663\n",
      "Processing 108000 of 184663\n",
      "Processing 109000 of 184663\n",
      "Processing 110000 of 184663\n",
      "Processing 111000 of 184663\n",
      "Processing 112000 of 184663\n",
      "Processing 113000 of 184663\n",
      "Processing 114000 of 184663\n",
      "Processing 115000 of 184663\n",
      "Processing 116000 of 184663\n",
      "Processing 117000 of 184663\n",
      "Processing 118000 of 184663\n",
      "Processing 119000 of 184663\n",
      "Processing 120000 of 184663\n",
      "Processing 121000 of 184663\n",
      "Processing 122000 of 184663\n",
      "Processing 123000 of 184663\n",
      "Processing 124000 of 184663\n",
      "Processing 125000 of 184663\n",
      "Processing 126000 of 184663\n",
      "Processing 127000 of 184663\n",
      "Processing 128000 of 184663\n",
      "Processing 129000 of 184663\n",
      "Processing 130000 of 184663\n",
      "Processing 131000 of 184663\n",
      "Processing 132000 of 184663\n",
      "Processing 133000 of 184663\n",
      "Processing 134000 of 184663\n",
      "Processing 135000 of 184663\n",
      "Processing 136000 of 184663\n",
      "Processing 137000 of 184663\n",
      "Processing 138000 of 184663\n",
      "Processing 139000 of 184663\n",
      "Processing 140000 of 184663\n",
      "Processing 141000 of 184663\n",
      "Processing 142000 of 184663\n",
      "Processing 143000 of 184663\n",
      "Processing 144000 of 184663\n",
      "Processing 145000 of 184663\n",
      "Processing 146000 of 184663\n",
      "Processing 147000 of 184663\n",
      "Processing 148000 of 184663\n",
      "Processing 149000 of 184663\n",
      "Processing 150000 of 184663\n",
      "Processing 151000 of 184663\n",
      "Processing 152000 of 184663\n",
      "Processing 153000 of 184663\n",
      "Processing 154000 of 184663\n",
      "Processing 155000 of 184663\n",
      "Processing 156000 of 184663\n",
      "Processing 157000 of 184663\n",
      "Processing 158000 of 184663\n",
      "Processing 159000 of 184663\n",
      "Processing 160000 of 184663\n",
      "Processing 161000 of 184663\n",
      "Processing 162000 of 184663\n",
      "Processing 163000 of 184663\n",
      "Processing 164000 of 184663\n",
      "Processing 165000 of 184663\n",
      "Processing 166000 of 184663\n",
      "Processing 167000 of 184663\n",
      "Processing 168000 of 184663\n",
      "Processing 169000 of 184663\n",
      "Processing 170000 of 184663\n",
      "Processing 171000 of 184663\n",
      "Processing 172000 of 184663\n",
      "Processing 173000 of 184663\n",
      "Processing 174000 of 184663\n",
      "Processing 175000 of 184663\n",
      "Processing 176000 of 184663\n",
      "Processing 177000 of 184663\n",
      "Processing 178000 of 184663\n",
      "Processing 179000 of 184663\n",
      "Processing 180000 of 184663\n",
      "Processing 181000 of 184663\n",
      "Processing 182000 of 184663\n",
      "Processing 183000 of 184663\n",
      "Processing 184000 of 184663\n",
      "predictions.predict saved\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def make_test_pred_and_save(df_test_feat):\n",
    "    supergroups_list = []\n",
    "    groups_list = []\n",
    "    modules_list = []\n",
    "    brands_list = []\n",
    "    indoml_id_list = range(0, len(df_test_feat))\n",
    "    length_df = df_test_feat.shape[0]\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for i in range(length_df):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Processing {i} of {length_df - 1}\")\n",
    "            inputs = tokenizer(df_test_feat.iloc[i].description, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n",
    "            # Append predictions to respective lists\n",
    "            supergroups_list.append(predictions[0])\n",
    "            groups_list.append(predictions[1])\n",
    "            modules_list.append(predictions[2])\n",
    "            brands_list.append(predictions[3])\n",
    "        \n",
    "        try:\n",
    "            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in supergroups: {e}\")\n",
    "            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n",
    "        try:\n",
    "            groups_names = group_encoder.inverse_transform(groups_list)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in groups: {e}\")\n",
    "            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n",
    "        try:\n",
    "            modules_names = module_encoder.inverse_transform(modules_list)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in modules: {e}\")\n",
    "            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n",
    "        try:\n",
    "            brands_names = brand_encoder.inverse_transform(brands_list)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in brands: {e}\")\n",
    "            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n",
    "        # Create a DataFrame with predictions\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'indoml_id': indoml_id_list,\n",
    "            'supergroup': supergroups_names,\n",
    "            'group': groups_names,\n",
    "            'module': modules_names,\n",
    "            'brand': brands_names\n",
    "        })\n",
    "        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n",
    "        print(\"predictions.predict saved\")\n",
    "print(make_test_pred_and_save(df_test_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757610c4",
   "metadata": {
    "papermill": {
     "duration": 0.027428,
     "end_time": "2024-10-28T17:54:46.949814",
     "exception": false,
     "start_time": "2024-10-28T17:54:46.922386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedd3bd",
   "metadata": {
    "papermill": {
     "duration": 0.027451,
     "end_time": "2024-10-28T17:54:47.004822",
     "exception": false,
     "start_time": "2024-10-28T17:54:46.977371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5779523,
     "sourceId": 9642895,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920554,
     "sourceId": 9738545,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26848.544956,
   "end_time": "2024-10-28T17:54:49.851932",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-28T10:27:21.306976",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "092c541df4d64ac5b585aaabfa68abc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0986adb3f4bb4673a70755ef0c679571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0ea7783f72f14d189643617c75e5f81a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "188a184bb6ad44af83851c907084b968": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23f828335c2647e7a9171a340d1cb716",
       "placeholder": "",
       "style": "IPY_MODEL_792d0d1ab1f34056966b0e6c20cd31dc",
       "value": "model.safetensors:100%"
      }
     },
     "1ccd0ee80aa44af197960df8f875ed20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_092c541df4d64ac5b585aaabfa68abc8",
       "placeholder": "",
       "style": "IPY_MODEL_1e505cd7ab854bc6a8ca506aabc0c2ee",
       "value": "615/615[00:00&lt;00:00,50.4kB/s]"
      }
     },
     "1d7df1f05b8648e0a0218674eecc66d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ae9a557a20e84e5093658acdd6911c09",
       "placeholder": "",
       "style": "IPY_MODEL_a4007ad6c9234b80b484071173bc196a",
       "value": "1.12G/1.12G[00:05&lt;00:00,194MB/s]"
      }
     },
     "1e505cd7ab854bc6a8ca506aabc0c2ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "23f828335c2647e7a9171a340d1cb716": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b0e42fb7f974b938dd9a4ecde52f784": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46b46df446964445a73afae2cb5e3b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0ea7783f72f14d189643617c75e5f81a",
       "placeholder": "",
       "style": "IPY_MODEL_2b0e42fb7f974b938dd9a4ecde52f784",
       "value": "config.json:100%"
      }
     },
     "792d0d1ab1f34056966b0e6c20cd31dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7df13bc5de9848a486d29fba96718d47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8cb2e72795f64a86b7b91768cf737842": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "970682bfe48842c9bb18a45594240851": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ece24c2ec0c44a29b27acff3b2e3c024",
       "max": 615.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7df13bc5de9848a486d29fba96718d47",
       "value": 615.0
      }
     },
     "a4007ad6c9234b80b484071173bc196a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ae9a557a20e84e5093658acdd6911c09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b254c13a135947089958ec76dc1c9e5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_46b46df446964445a73afae2cb5e3b21",
        "IPY_MODEL_970682bfe48842c9bb18a45594240851",
        "IPY_MODEL_1ccd0ee80aa44af197960df8f875ed20"
       ],
       "layout": "IPY_MODEL_8cb2e72795f64a86b7b91768cf737842"
      }
     },
     "d440298975b049939921e9ea18ed13a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0d6b50821204d62a05717e476e1ab62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_188a184bb6ad44af83851c907084b968",
        "IPY_MODEL_ef574ac705dc48e8bd046b1b623d1588",
        "IPY_MODEL_1d7df1f05b8648e0a0218674eecc66d7"
       ],
       "layout": "IPY_MODEL_f8e56e31f7664fadaf806f909bcabab0"
      }
     },
     "ece24c2ec0c44a29b27acff3b2e3c024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef574ac705dc48e8bd046b1b623d1588": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d440298975b049939921e9ea18ed13a2",
       "max": 1115567652.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0986adb3f4bb4673a70755ef0c679571",
       "value": 1115567652.0
      }
     },
     "f8e56e31f7664fadaf806f909bcabab0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
